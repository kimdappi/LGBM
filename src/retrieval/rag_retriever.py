"""
RAG Retriever - FAISS ê¸°ë°˜ ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ + LLM ì§„ë‹¨ í•„í„°ë§ + Reranking
ì´ë¯¸ ìƒì„±ëœ FAISS DBì™€ metadata.pkl ì‚¬ìš©

3ë‹¨ê³„ ê²€ìƒ‰:
  Stage 1: MedCPT Query Encoder + FAISS â†’ top-10 í›„ë³´ (ë¹ ë¦„)
  Stage 2: LLM ì§„ë‹¨ ì¶”ì¶œ â†’ ìœ ì‚¬ ì§„ë‹¨ í•„í„°ë§ (ì •í™•í•œ recall)
  Stage 3: Cross-encoder Reranking â†’ top-k ìµœì¢… (ì •í™•í•œ ranking)
  
ì„ë² ë”© ëª¨ë¸ ì˜µì…˜:
  - MedCPT (í˜„ì¬): ë¹„ëŒ€ì¹­ ì¸ì½”ë”, PubMed ê²€ìƒ‰ íŠ¹í™”
  - BioLORD: ë‹¨ì¼ ì¸ì½”ë”, UMLS ê¸°ë°˜ ì„ìƒ ê°œë… ìœ ì‚¬ì„±
"""

import numpy as np
import pickle
import faiss
import torch
import requests
import json
import os
from pathlib import Path
from typing import List, Dict, Optional, Set
from transformers import AutoTokenizer, AutoModel
from dotenv import load_dotenv

# .env ë¡œë“œ
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# Reranker lazy import
_reranker = None #ì´ˆê¸°í™”

_reranker_model_name = "BAAI/bge-reranker-v2-m3"
# _reranker_model_name = "BAAI/bge-reranker-base"

# ì˜ë£Œ íŠ¹í™” ë¦¬ë­ì»¤ ì˜µì…˜ (í–¥í›„ ê³ ë ¤):
# - MedCPTëŠ” bi-encoderë§Œ ìˆê³  cross-encoderëŠ” ì—†ìŒ
# - BioLinkBERT, PubMedBERT ê¸°ë°˜ cross-encoder ì§ì ‘ íŒŒì¸íŠœë‹ í•„ìš”
# â†’ í˜„ì¬ëŠ” BGE-Reranker + M&M íŠ¹í™” instruction ì‚¬ìš©

# Reranker instruction - M&M ì»¨í¼ëŸ°ìŠ¤ ëª©ì ì— ë§ì¶˜ ì¼€ì´ìŠ¤ ê²€ìƒ‰
# M&Mì˜ ëª©í‘œ: ë¹„íŒì  ë„ì¶œ ë° í•´ê²°ì±… í•™ìŠµ
# â†’ ë¹„ìŠ·í•œ ì„ìƒ ìƒí™©ì—ì„œ ì§„ë‹¨/ì¹˜ë£Œ ì˜ì‚¬ê²°ì •ì— êµí›ˆì„ ì¤„ ìˆ˜ ìˆëŠ” ì¼€ì´ìŠ¤
RERANKER_INSTRUCTION = "Find cases with similar clinical presentations that provide lessons about diagnosis, treatment decisions, or potential complications"

def get_reranker():
    """Reranker ëª¨ë¸ lazy loading (FlagReranker with instruction support)"""
    global _reranker
    if _reranker is None:
        try:
            from FlagEmbedding import FlagReranker
            print(f"[Reranker] Loading {_reranker_model_name} (instruction-tuned)...")
            _reranker = FlagReranker(_reranker_model_name, use_fp16=True)
            print(f"[Reranker] Loaded successfully")
            print(f"[Reranker] Instruction: '{RERANKER_INSTRUCTION}'")
        except ImportError:
            print("[Warning] FlagEmbedding not installed. Trying sentence-transformers fallback...")
            try:
                from sentence_transformers import CrossEncoder
                print(f"[Reranker] Loading {_reranker_model_name} via CrossEncoder (no instruction)...")
                _reranker = ("crossencoder", CrossEncoder(_reranker_model_name, max_length=512))
                print(f"[Reranker] Loaded (fallback mode, instruction not supported)")
            except ImportError:
                print("[Warning] Neither FlagEmbedding nor sentence-transformers installed.")
                print("  Install: pip install FlagEmbedding")
                _reranker = "disabled"
        except Exception as e:
            print(f"[Warning] Failed to load reranker: {e}")
            _reranker = "disabled"
    return _reranker if _reranker != "disabled" else None


class DiagnosisExtractor:
    """LLM ê¸°ë°˜ ì§„ë‹¨ ì¶”ì¶œê¸° - ì…ì› ì£¼ ì›ì¸ vs ë™ë°˜ ì§ˆí™˜ êµ¬ë¶„"""
    
    def __init__(self, model: str = "gpt-4o-mini"):
        self.model = model
        self.api_url = "https://api.openai.com/v1/chat/completions"
        self.api_key = os.environ.get("OPENAI_API_KEY", "")
        self._cache = {}  # ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    
    def extract(self, text: str, use_cache: bool = True) -> Dict[str, List[str]]:
        """
        ì„ìƒ í…ìŠ¤íŠ¸ì—ì„œ ì…ì› ì£¼ ì›ì¸ê³¼ ë™ë°˜ ì§ˆí™˜ì„ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ
        
        Args:
            text: ì„ìƒ í…ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            {
                'chief_complaint': ['Shortness of breath'],
                'primary_diagnosis': ['COPD EXACERBATION', 'RESPIRATORY FAILURE'],
                'comorbidities': ['CAD', 'ATRIAL FIBRILLATION', 'HYPERTENSION']
            }
        """
        # ìºì‹œ í™•ì¸
        cache_key = hash(text)
        if use_cache and cache_key in self._cache:
            return self._cache[cache_key]
        
        if not self.api_key:
            print("[DiagnosisExtractor] API key not found")
            return {'chief_complaint': [], 'primary_diagnosis': [], 'comorbidities': []}
        
        prompt = f"""Analyze this clinical note and extract diagnoses into THREE categories.

Definitions:
- chief_complaint: presenting symptom/reason for visit
- primary_diagnosis: main cause of THIS hospitalization
- comorbidities: pre-existing conditions, not the main cause

Return ONLY JSON:
{{"chief_complaint": [...], "primary_diagnosis": [...], "comorbidities": [...]}}

Text:
{text}"""

        try:
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 500
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            content = result["choices"][0]["message"]["content"].strip()
            
            # JSON íŒŒì‹±
            extracted = self._parse_structured_diagnoses(content)
            
            # ìºì‹œ ì €ì¥
            if use_cache:
                self._cache[cache_key] = extracted
            
            return extracted
            
        except Exception as e:
            print(f"[DiagnosisExtractor] LLM failed: {e}")
            return {'chief_complaint': [], 'primary_diagnosis': [], 'comorbidities': []}
    
    def _parse_structured_diagnoses(self, content: str) -> Dict[str, List[str]]:
        """LLM ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì§„ë‹¨ ì •ë³´ íŒŒì‹±"""
        default = {'chief_complaint': [], 'primary_diagnosis': [], 'comorbidities': []}
        
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            if "```" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
            
            data = json.loads(content)
            
            result = {}
            for key in ['chief_complaint', 'primary_diagnosis', 'comorbidities']:
                if key in data and isinstance(data[key], list):
                    result[key] = [d.upper().strip() for d in data[key] if isinstance(d, str)]
                else:
                    result[key] = []
            
            return result
            
        except:
            return default
    
    def is_similar(
        self, 
        extracted1: Dict[str, List[str]], 
        extracted2: Dict[str, List[str]], 
        match_type: str = "primary"
    ) -> bool:
        """
        ë‘ í™˜ìê°€ ì„ìƒì ìœ¼ë¡œ ìœ ì‚¬í•œì§€ í™•ì¸ (ì…ì› ì£¼ ì›ì¸ ê¸°ì¤€)
        
        Args:
            extracted1: ì²« ë²ˆì§¸ í™˜ìì˜ ì¶”ì¶œ ê²°ê³¼
            extracted2: ë‘ ë²ˆì§¸ í™˜ìì˜ ì¶”ì¶œ ê²°ê³¼
            match_type: ë§¤ì¹­ ê¸°ì¤€
                - "primary": primary_diagnosisë§Œ ë¹„êµ (ê¶Œì¥)
                - "chief": chief_complaintë„ ë¹„êµ
                - "any": ëª¨ë“  í•„ë“œ ë¹„êµ (ê¸°ì¡´ ë°©ì‹)
        
        Returns:
            ìœ ì‚¬ ì—¬ë¶€
        """
        # ì´ì „ ë²„ì „ í˜¸í™˜ì„±: Listê°€ ë“¤ì–´ì˜¤ë©´ primary_diagnosisë¡œ ì²˜ë¦¬
        if isinstance(extracted1, list):
            extracted1 = {'primary_diagnosis': extracted1, 'chief_complaint': [], 'comorbidities': []}
        if isinstance(extracted2, list):
            extracted2 = {'primary_diagnosis': extracted2, 'chief_complaint': [], 'comorbidities': []}
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í†µê³¼ (í•„í„°ë§ ê±´ë„ˆëœ€)
        if not extracted1.get('primary_diagnosis') or not extracted2.get('primary_diagnosis'):
            return True
        
        # ë™ì˜ì–´ í™•ì¥
        def expand_with_synonyms(terms: List[str]) -> Set[str]:
            synonyms = {
                "CHF": "HEART FAILURE",
                "AFIB": "ATRIAL FIBRILLATION", 
                "CAD": "CORONARY ARTERY DISEASE",
                "MI": "MYOCARDIAL INFARCTION",
                "CVA": "STROKE",
                "COPD EXACERBATION": "COPD",
                "AECOPD": "COPD EXACERBATION",
                "RESPIRATORY FAILURE": "RESPIRATORY DISTRESS",
                "LIVER FAILURE": "HEPATIC FAILURE",
                "DECOMPENSATED CIRRHOSIS": "CIRRHOSIS",
            }
            
            expanded = set(t.upper().strip() for t in terms)
            for abbrev, full in synonyms.items():
                if abbrev in expanded:
                    expanded.add(full)
                if full in expanded:
                    expanded.add(abbrev)
            return expanded
        
        # Primary diagnosis ë¹„êµ (í•µì‹¬!)
        primary1 = expand_with_synonyms(extracted1.get('primary_diagnosis', []))
        primary2 = expand_with_synonyms(extracted2.get('primary_diagnosis', []))
        
        primary_overlap = len(primary1 & primary2)
        
        if match_type == "primary":
            return primary_overlap >= 1
        
        # Chief complaintë„ ë¹„êµ
        if match_type == "chief":
            chief1 = expand_with_synonyms(extracted1.get('chief_complaint', []))
            chief2 = expand_with_synonyms(extracted2.get('chief_complaint', []))
            chief_overlap = len(chief1 & chief2)
            return primary_overlap >= 1 or chief_overlap >= 1
        
        # Any: ë™ë°˜ ì§ˆí™˜ë„ í¬í•¨ (ê¸°ì¡´ ë°©ì‹, ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
        all1 = primary1 | expand_with_synonyms(extracted1.get('comorbidities', []))
        all2 = primary2 | expand_with_synonyms(extracted2.get('comorbidities', []))
        return len(all1 & all2) >= 1


BASE_DIR = Path(__file__).resolve().parents[2]  # project root
DEFAULT_DB_PATH = BASE_DIR / "data" / "vector_db"

class VectorDBManager:
    """FAISS ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤ - ê¸°ì¡´ DB ë¡œë“œ"""
    # MedCPT (í˜„ì¬ ì‚¬ìš©)
    EMBEDDING_MODEL = 'ncbi/MedCPT-Query-Encoder'
    
    # BioLORD (ëŒ€ì•ˆ) - ìœ„ ì¤„ ì£¼ì„ì²˜ë¦¬ í›„ ì•„ë˜ ì£¼ì„í•´ì œ
    # EMBEDDING_MODEL = 'FremyCompany/BioLORD-2023-M'
    
    def __init__(self, embedding_model: str = None, db_path: str = '../data/vector_db'):
        """
        ì„ë² ë”© ëª¨ë¸ ì˜µì…˜:
        - ncbi/MedCPT-Query-Encoder: ê²€ìƒ‰ ì¿¼ë¦¬ìš© (DBëŠ” Article-Encoderë¡œ êµ¬ì¶•)
        - FremyCompany/BioLORD-2023-M: ë‹¨ì¼ ì¸ì½”ë”, ì„ìƒ ê°œë… ìœ ì‚¬ì„± íŠ¹í™”
        """
        self.embedding_model = embedding_model or self.EMBEDDING_MODEL
        self.db_path = Path(DEFAULT_DB_PATH)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # ì¿¼ë¦¬ ì„ë² ë”©ìš©
        self.tokenizer = None
        self.model = None
        # FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° 
        self.index = None
        self.metadata = []
        
    def load(self):
        """ê¸°ì¡´ ë²¡í„° DB ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        print(f"\n[VectorDBManager]")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {self.embedding_model}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.embedding_model)
        self.model = AutoModel.from_pretrained(self.embedding_model).to(self.device)
        self.model.eval()
        index_path = self.db_path / "faiss_index.idx"
        self.index = faiss.read_index(str(index_path))
        
        # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ (data/vector_db/metadata.pkl)
        #    ì´ë¯¸ text í¬í•¨í•œ ì „ì²´ recordê°€ ì €ì¥ë˜ì–´ ìˆìŒ
        metadata_path = self.db_path / "metadata.pkl"

        print(f"  - ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
        with open(metadata_path, 'rb') as f:
            self.metadata = pickle.load(f)
        
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê±´ì˜ ì¼€ì´ìŠ¤")
    
    def embed_text(self, text: str, max_length: int = 512) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ BioBERTë¡œ ì„ë² ë”© (ì¿¼ë¦¬ìš©)"""
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        processed_text = text if text and text.strip() else ' '
        
        # í† í°í™”
        inputs = self.tokenizer(
            processed_text,
            return_tensors='pt',
            max_length=max_length,
            truncation=True,
            padding=True
        ).to(self.device)
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            outputs = self.model(**inputs)
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        return embedding.astype(np.float32)
    
    def search(
        self, 
        query_text: str, 
        top_k: int = 3, 
        exclude_id: str = None,
        use_reranker: bool = True,
        use_diagnosis_filter: bool = True,
        rerank_top_n: int = 10  # ì‘ì€ DBìš©
    ) -> List[Dict]:
        """
        ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ - 3ë‹¨ê³„ ê²€ìƒ‰ (FAISS + ì§„ë‹¨ í•„í„°ë§ + Reranking)
        
        Args:
            query_text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            exclude_id: ì œì™¸í•  í™˜ì ID (ìê¸° ìì‹  ì œì™¸ìš©)
            use_reranker: Reranker ì‚¬ìš© ì—¬ë¶€
            use_diagnosis_filter: LLM ì§„ë‹¨ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            rerank_top_n: Stage 1ì—ì„œ ê°€ì ¸ì˜¬ í›„ë³´ ìˆ˜
        
        Returns:
            ìœ ì‚¬ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (text + ì „ì²´ metadata + similarity í¬í•¨)
        """
        
        print(f"\n[Stage 1] FAISS ê²€ìƒ‰: top-{rerank_top_n} í›„ë³´")
        
        # ìê¸° ìì‹ ì´ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜´
        fetch_k = rerank_top_n + 10 if exclude_id else rerank_top_n
        
        query_vector = self.embed_text(query_text)
        faiss.normalize_L2(query_vector)
        similarities, indices = self.index.search(query_vector, fetch_k)
        
        # ê²°ê³¼ êµ¬ì„± (ìê¸° ìì‹  ì œì™¸)
        candidates = []
        for dist, idx in zip(similarities[0], indices[0]):
            record = self.metadata[idx].copy()
            record['similarity'] = float(dist)
            
            # ìê¸° ìì‹  ì œì™¸
            if exclude_id and str(record.get('id')) == str(exclude_id):
                continue
                
            candidates.append(record)
            
            if len(candidates) >= rerank_top_n:
                break
        
        print(f"  â†’ {len(candidates)}ê°œ í›„ë³´ ê²€ìƒ‰ë¨")
        
        # ì§„ë‹¨ í•„í„°ë§
        if use_diagnosis_filter and len(candidates) > top_k:
            candidates = self._filter_by_diagnosis(query_text, candidates)
        
        # Reranker
        if use_reranker and len(candidates) > top_k:
            candidates = self._rerank(query_text, candidates, top_k)
        else:
            candidates = candidates[:top_k]
        
        return candidates
    
    def _filter_by_diagnosis(self, query_text: str, candidates: List[Dict]) -> List[Dict]:
        """
        LLMìœ¼ë¡œ ì§„ë‹¨ ì¶”ì¶œ í›„ ìœ ì‚¬ ì§„ë‹¨ í•„í„°ë§
        
        í•µì‹¬: PRIMARY DIAGNOSIS (ì…ì› ì£¼ ì›ì¸)ë§Œ ë¹„êµ
              COMORBIDITIES (ë™ë°˜ ì§ˆí™˜)ëŠ” ë§¤ì¹­ì—ì„œ ì œì™¸
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidates: Stage 1 í›„ë³´ë“¤
        
        Returns:
            ì…ì› ì£¼ ì›ì¸ì´ ìœ ì‚¬í•œ í›„ë³´ë“¤ë§Œ í•„í„°ë§
        """
        print(f"\n[Stage 2] LLM ì§„ë‹¨ í•„í„°ë§ (Primary Diagnosis ê¸°ì¤€)")
        
        extractor = DiagnosisExtractor()
        
        # ì¿¼ë¦¬ í™˜ìì˜ ì§„ë‹¨ ì¶”ì¶œ (êµ¬ì¡°í™”ëœ í˜•íƒœ)
        query_extracted = extractor.extract(query_text)
        print(f"  ì¿¼ë¦¬ í™˜ì:")
        print(f"    - Primary Diagnosis: {query_extracted.get('primary_diagnosis', [])} (ê²€ìƒ‰ ì‚¬ìš© âœ…)")
        print(f"    - Chief Complaint: {query_extracted.get('chief_complaint', [])} (ì°¸ê³ ìš©)")
        print(f"    - Comorbidities: {query_extracted.get('comorbidities', [])} (ë§¤ì¹­ ì œì™¸)")
        
        if not query_extracted.get('primary_diagnosis'):
            print("  â†’ Primary diagnosis ì¶”ì¶œ ì‹¤íŒ¨, í•„í„°ë§ ê±´ë„ˆëœ€")
            return candidates
        
        # í›„ë³´ë“¤ í•„í„°ë§ (Primary Diagnosis ê¸°ì¤€)
        filtered = []
        for c in candidates:
            candidate_extracted = extractor.extract(c.get('text', ''))
            
            # Primary Diagnosisë§Œ ë¹„êµ (í•µì‹¬!)
            if extractor.is_similar(query_extracted, candidate_extracted, match_type="primary"):
                c['extracted_diagnoses'] = candidate_extracted
                filtered.append(c)
        
        print(f"  â†’ {len(candidates)}ê°œ â†’ {len(filtered)}ê°œ (Primary Diagnosis ì¼ì¹˜)")
        
        # í•„í„°ë§ëœ ì¼€ì´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
        for i, c in enumerate(filtered[:5]):
            primary = c.get('extracted_diagnoses', {}).get('primary_diagnosis', [])
            print(f"    {i+1}. ID={c.get('id')}: {primary}")
        
        # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ìœ ì§€
        if len(filtered) < 3:
            print(f"  â†’ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(filtered)}ê°œ), ì›ë³¸ ìƒìœ„ 10ê°œ ìœ ì§€")
            return candidates[:10]
        
        return filtered
    
    def _rerank(self, query_text: str, candidates: List[Dict], top_k: int) -> List[Dict]:
        """
        Instruction-tuned Cross-encoderë¡œ í›„ë³´ reranking
        
        Instruction: "Find cases that share the same primary disease mechanism"
        â†’ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ì•„ë‹Œ ì§ˆí™˜ ë©”ì»¤ë‹ˆì¦˜ ê¸°ë°˜ ìœ ì‚¬ì„± í‰ê°€
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidates: Stage 1ì—ì„œ ê²€ìƒ‰ëœ í›„ë³´ë“¤
            top_k: ìµœì¢… ë°˜í™˜í•  ê°œìˆ˜
        
        Returns:
            Rerankingëœ ìƒìœ„ top_k ì¼€ì´ìŠ¤
        """
        reranker = get_reranker()
        
        if reranker is None:
            # Reranker ì‚¬ìš© ë¶ˆê°€ ì‹œ ì›ë˜ ìˆœì„œ ìœ ì§€
            print("[Reranker] Disabled, using original order")
            return candidates[:top_k]
        
        # Query-Candidate ìŒ ìƒì„±
        pairs = []
        for c in candidates:
            candidate_text = c.get('text', '')
            pairs.append([query_text, candidate_text])
        
        # Reranker íƒ€ì…ì— ë”°ë¥¸ ì ìˆ˜ ì‚°ì¶œ
        print(f"[Reranker] Scoring {len(pairs)} candidates...")
        
        if isinstance(reranker, tuple) and reranker[0] == "crossencoder":
            # Fallback: CrossEncoder (instruction ë¯¸ì§€ì›)
            print("[Reranker] Using CrossEncoder fallback (no instruction)")
            scores = reranker[1].predict(pairs)
        else:
            # FlagReranker with instruction
            print(f"[Reranker] Instruction: '{RERANKER_INSTRUCTION}'")
            scores = reranker.compute_score(
                pairs,
                normalize=False  # Raw logit ì‚¬ìš© (ìˆœì„œë§Œ ì¤‘ìš”, ì ˆëŒ“ê°’ ë¬´ì˜ë¯¸)
            )
            # compute_scoreëŠ” ë‹¨ì¼ ìŒì´ë©´ float, ì—¬ëŸ¬ ìŒì´ë©´ list ë°˜í™˜
            if not isinstance(scores, list):
                scores = [scores]
        
        # ì ìˆ˜ë¡œ ì¬ì •ë ¬ (ìˆœì„œë§Œ ë°”ê¾¸ê³  similarityëŠ” FAISS ì›ë³¸ ìœ ì§€)
        for i, c in enumerate(candidates):
            c['rerank_score'] = float(scores[i])
            # CRITICAL: similarityëŠ” FAISS ì›ë³¸ ìœ ì§€ (0.7 ì„ê³„ì¹˜ ì²´í¬ìš©)
            # ë¦¬ë­ì»¤ ìŠ¤ì½”ì–´ëŠ” ìˆœì„œ ì •ë ¬ì—ë§Œ ì‚¬ìš©
        
        # ğŸ” DEBUG: ì „ì²´ ë¦¬ë­ì»¤ ìŠ¤ì½”ì–´ ë¶„í¬ í™•ì¸
        all_rerank_scores = [c['rerank_score'] for c in candidates]
        print(f"[Reranker] Score ë¶„í¬: min={min(all_rerank_scores):.3f}, max={max(all_rerank_scores):.3f}, avg={sum(all_rerank_scores)/len(all_rerank_scores):.3f}")
        print(f"[Reranker] ì „ì²´ {len(candidates)}ê°œ ìŠ¤ì½”ì–´: {[f'{s:.3f}' for s in sorted(all_rerank_scores, reverse=True)]}")
        
        # ë†’ì€ rerank_score ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        print(f"[Reranker] Top-{top_k} selected (ìˆœì„œ=rerank, ìœ ì‚¬ë„=FAISS ì›ë³¸)")
        for i, c in enumerate(candidates[:top_k]):
            print(f"  {i+1}. ID={c.get('id')}, similarity={c['similarity']:.3f} (FAISS âœ…), rerank={c['rerank_score']:.3f} (ìˆœì„œ)")
        
        return candidates[:top_k]

class RAGRetriever:
    """ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ê¸° - cohort_data ë°˜í™˜"""
    
    def __init__(
        self,
        db_path: str = 'data/vector_db',
        embedding_model: str = 'ncbi/MedCPT-Query-Encoder'
    ):
        self.vector_db = VectorDBManager(
            embedding_model=embedding_model,
            db_path=db_path
        )
        self.is_loaded = False
    
    def load(self):
        """ë²¡í„° DB ë¡œë“œ"""
        self.vector_db.load()
        self.is_loaded = True
        print(f"\n RAG Retriever ë¡œë“œ ì™„ë£Œ: {len(self.vector_db.metadata)}ê±´")
    
    def retrieve(
        self, 
        query_text: str, 
        top_k: int = 3,
        exclude_id: str = None,
        use_reranker: bool = True,
        use_diagnosis_filter: bool = True,
        rerank_top_n: int = 10
    ) -> Dict:
        """
        ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ ë° cohort_data ë°˜í™˜ (3ë‹¨ê³„ ê²€ìƒ‰)
        
        Args:
            query_text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            exclude_id: ì œì™¸í•  í™˜ì ID (ìê¸° ìì‹  ì œì™¸)
            use_reranker: Reranker ì‚¬ìš© ì—¬ë¶€
            use_diagnosis_filter: LLM ì§„ë‹¨ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            rerank_top_n: Stage 1ì—ì„œ ê°€ì ¸ì˜¬ í›„ë³´ ìˆ˜
        """
        if not self.is_loaded:
            self.load()
        
        # ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ (3ë‹¨ê³„: FAISS â†’ ì§„ë‹¨í•„í„°ë§ â†’ Reranking)
        similar_cases = self.vector_db.search(
            query_text, 
            top_k=top_k,
            exclude_id=exclude_id,
            use_reranker=use_reranker,
            use_diagnosis_filter=use_diagnosis_filter,
            rerank_top_n=rerank_top_n
        )
        
        # cohort_data êµ¬ì„±
        cohort_data = {
            'similar_cases': similar_cases,
            'stats': self._calculate_stats(similar_cases)
        }
        
        return cohort_data
    
    def _calculate_stats(self, similar_cases: List[Dict]) -> Dict:
        """ìœ ì‚¬ ì¼€ì´ìŠ¤ í†µê³„ ê³„ì‚°"""
        if not similar_cases:
            return {'total': 0, 'expired_count': 0, 'survival_rate': None}
        
        total = len(similar_cases)
        expired_count = sum(
            1 for c in similar_cases 
            if c.get('status', '').lower() in ['expired', 'died', 'death']
        )
        survival_rate = (total - expired_count) / total if total > 0 else None
        
        return {
            'total': total,
            'expired_count': expired_count,
            'survival_rate': survival_rate
        }
    
    def retrieve_with_patient(
        self, 
        patient_data: Dict, 
        top_k: int = 3,
        use_reranker: bool = True,
        use_diagnosis_filter: bool = True,
        rerank_top_n: int = 10
    ) -> Dict:
        """
        í™˜ì ë°ì´í„°ë¡œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ ë° cohort_data ë°˜í™˜
        
        Args:
            patient_data: í™˜ì ì •ë³´ ë”•ì…”ë„ˆë¦¬
                {
                    'id': str,
                    'status': str,
                    'sex': str,
                    'age': int,
                    'admission_type': str,
                    'admission_location': str,
                    'discharge_location': str,
                    'arrival_transport': str,
                    'text': str
                }
            top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            use_reranker: Reranker ì‚¬ìš© ì—¬ë¶€
            use_diagnosis_filter: LLM ì§„ë‹¨ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            rerank_top_n: Stage 1ì—ì„œ ê°€ì ¸ì˜¬ í›„ë³´ ìˆ˜
        
        Returns:
            cohort_data (ìœ„ì™€ ë™ì¼)
        """
        # 'text' ë˜ëŠ” 'clinical_text' í‚¤ ëª¨ë‘ ì§€ì›
        query_text = patient_data.get('text', '') or patient_data.get('clinical_text', '')
        patient_id = patient_data.get('id') or patient_data.get('patient_id')  # ìê¸° ìì‹  ì œì™¸ìš©
        
        return self.retrieve(
            query_text, 
            top_k=top_k,
            exclude_id=patient_id,
            use_reranker=use_reranker,
            use_diagnosis_filter=use_diagnosis_filter,
            rerank_top_n=rerank_top_n
        )